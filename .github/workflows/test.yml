name: Test Suite

# This comprehensive test suite runs only on:
# 1. Version tags (v*.*.* - for release testing)
# 2. Manual dispatch (workflow_dispatch - for on-demand testing)
# This prevents excessive CI usage while ensuring thorough testing before releases.

on:
  push:
    tags:
      - "v*.*.*"
  workflow_dispatch:

jobs:
  test:
    # Main test job - runs all 300+ tests across unit, integration, performance, and UI categories
    # All tests have been modernized and are now passing without exclusions
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.10", "3.11"]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-xdist pytest-mock pytest-benchmark
    
    - name: Create test secrets file
      run: |
        mkdir -p .streamlit
        cat > .streamlit/secrets.toml << EOF
        # This an empty secrets.toml that
        # allows the application to run in desktop mode.
        
        [openai]
        api_key = ""
        
        [auth]
        redirect_uri = ""
        cookie_secret = "ci-test-placeholder-secret-12345678901234567890"
        client_id = ""
        client_secret = ""
        server_metadata_url = ""
        
        [firestore]
        key_dict = { }
        EOF
    
    - name: Run linting
      run: |
        pip install flake8
        flake8 webapp/ tests/ --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 webapp/ tests/ --count --exit-zero --max-complexity=10 --max-line-length=120 --statistics
    
    - name: Run unit tests
      run: |
        pytest tests/unit/ -v --tb=short --cov=webapp --cov-report=xml --cov-report=term-missing -n auto
    
    - name: Run integration tests
      run: |
        pytest tests/integration/ -v --tb=short
    
    - name: Run performance tests
      run: |
        pytest tests/performance/ -v --tb=short
    
    - name: Run streamlit UI tests
      run: |
        pytest tests/streamlit/ -v --tb=short
    
    - name: Run full test suite (verification)
      # Final verification: all 300+ tests including unit, integration, performance, and UI tests
      # Tests cover: session persistence, authorization, data management, AI/LLM integration,
      # corpus processing, error handling, monitoring, plotting, and streamlit workflows
      run: |
        pytest tests/ -v --tb=short --cov=webapp --cov-report=term-missing
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        fail_ci_if_error: false
        verbose: true

  performance-tests:
    runs-on: ubuntu-latest
    needs: test
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-benchmark
    
    - name: Create test secrets file
      run: |
        mkdir -p .streamlit
        cat > .streamlit/secrets.toml << EOF
        # This an empty secrets.toml that
        # allows the application to run in desktop mode.
        
        [openai]
        api_key = ""
        
        [auth]
        redirect_uri = ""
        cookie_secret = "ci-perf-test-secret-12345678901234567890"
        client_id = ""
        client_secret = ""
        server_metadata_url = ""
        
        [firestore]
        key_dict = { }
        EOF
    
    - name: Run performance benchmarks
      run: |
        pytest tests/performance/ -v --tb=short --benchmark-only --benchmark-json=performance-results.json
    
    - name: Store performance results
      uses: actions/upload-artifact@v4
      with:
        name: performance-results
        path: performance-results.json
        retention-days: 30

  security-scan:
    runs-on: ubuntu-latest
    needs: test
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"
    
    - name: Install security tools
      run: |
        python -m pip install --upgrade pip
        pip install safety bandit
    
    - name: Run safety check for known vulnerabilities
      run: |
        safety check -r requirements.txt --json --output safety-results.json || true
    
    - name: Run bandit security linter
      run: |
        bandit -r webapp/ -f json -o bandit-results.json || true
    
    - name: Upload security results
      uses: actions/upload-artifact@v4
      with:
        name: security-results
        path: |
          safety-results.json
          bandit-results.json
        retention-days: 30

  docker-test:
    runs-on: ubuntu-latest
    needs: test
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Build Docker image
      run: |
        docker build -t docuscope-ca-test .
    
    - name: Test Docker image
      run: |
        docker run --rm -d --name docuscope-test -p 8501:8501 docuscope-ca-test
        sleep 30
        curl -f http://localhost:8501/ || exit 1
        docker stop docuscope-test

  desktop-test:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ["3.10", "3.11"]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest
    
    - name: Create desktop test config
      shell: bash
      run: |
        mkdir -p .streamlit
        cat > .streamlit/secrets.toml << EOF
        # This an empty secrets.toml that
        # allows the application to run in desktop mode.
        
        [openai]
        api_key = ""
        
        [auth]
        redirect_uri = ""
        cookie_secret = "ci-desktop-test-secret-12345678901234567890"
        client_id = ""
        client_secret = ""
        server_metadata_url = ""
        
        [firestore]
        key_dict = { }
        EOF
    
    - name: Run desktop mode tests
      run: |
        pytest tests/unit/config/ tests/unit/auth/ -v
    
    - name: Test application startup (Unix)
      if: matrix.os != 'windows-latest'
      run: |
        timeout 30s python -c "
        import subprocess
        import time
        import sys
        
        proc = subprocess.Popen([sys.executable, '-m', 'streamlit', 'run', 'webapp/index.py', '--server.headless=true', '--server.port=8502'])
        time.sleep(20)
        proc.terminate()
        proc.wait()
        print('Application started successfully')
        " || echo "Startup test completed"
    
    - name: Test application startup (Windows)
      if: matrix.os == 'windows-latest'
      run: |
        python -c "
        import subprocess
        import time
        import sys
        
        proc = subprocess.Popen([sys.executable, '-m', 'streamlit', 'run', 'webapp/index.py', '--server.headless=true', '--server.port=8502'])
        time.sleep(15)
        proc.terminate()
        proc.wait()
        print('Application started successfully')
        "
      timeout-minutes: 2
